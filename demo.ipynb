{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 16443, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Script parses RAW_INPUT_PATH, which is a dump from dynalist, and outputs\n",
    "# two files: OUTPUT_TRAIN_PATH, OUTPUT_TEST_PATH, which are preprocessed sentences, split into train and test\n",
    "RAW_INPUT_PATH = 'data/research_dump.txt'\n",
    "TRAIN_PATH = 'data/train.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "MODEL_NAME = 'gpt2'\n",
    "\n",
    "\"\"\"\n",
    "@param sentence: string.\n",
    "@return bool. Whether or not the sentence should be included as part of training.\n",
    "\"\"\"\n",
    "def is_valid_sentence(sentence):\n",
    "    return len(sentence) > 10\n",
    "\n",
    "\"\"\"\n",
    "@param sentence: string.\n",
    "@return string. The preprocessed sentence.\n",
    "\"\"\"\n",
    "def preprocess_sentence(sentence):\n",
    "    return re.sub(r\"\\*\\*\", \"\", sentence).strip()\n",
    "\n",
    "sentences = []\n",
    "sentence_cnt = {}\n",
    "line_id = 0\n",
    "with open(RAW_INPUT_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line_id += 1\n",
    "        line_sentences = line.split(\"\\n\")\n",
    "        num_line_sentence = len(line_sentences)\n",
    "        sentence_cnt[num_line_sentence] = sentence_cnt.get(num_line_sentence, 0) + 1\n",
    "        for sentence in line_sentences:\n",
    "            # print(f\"{line_id}: {sentence}\")\n",
    "            sentence = preprocess_sentence(sentence)\n",
    "            if not is_valid_sentence(sentence):\n",
    "                continue\n",
    "            sentences.append(sentence)\n",
    "print(sentence_cnt)\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.10)\n",
    "\n",
    "with open(TRAIN_PATH, \"w\", encoding='utf-8') as outfile:\n",
    "    outfile.write(\"\\n\".join(train_sentences))\n",
    "    \n",
    "with open(TEST_PATH, \"w\", encoding='utf-8') as outfile:\n",
    "    outfile.write(\"\\n\".join(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15088\n"
     ]
    }
   ],
   "source": [
    "# 15K sentences\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=4)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=4)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(TEST_PATH, TEST_PATH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sjona\\anaconda3\\lib\\site-packages\\transformers\\modeling_auto.py:821: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sjona\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:259: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"model/finetuned-gpt2\"\n",
    "model = AutoModelWithLMHead.from_pretrained(MODEL_NAME)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR, #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='23325' max='23325' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23325/23325 1:01:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.203875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.108259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.001910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.939924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.023035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.874520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.874719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.739141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.800875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.816199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.799102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.725074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.645352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.798312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.690336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.126195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.450781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.446695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.548281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.521117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.524711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.363180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.503609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.499484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.491156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.599547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.449984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.439891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.403562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.354469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.373625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.711844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.500078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.487891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.490578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.486750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.518531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.502906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.582766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.491437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.602578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.431563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.550391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.571047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.514813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.453922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=23325, training_loss=4.615244172025723)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"model/finetuned-gpt2\"\n",
    "\n",
    "def generate_sentences(prompt, temperature, num_sample):\n",
    "    \"\"\"\n",
    "    @param prompt: string. The sentence to autocomplete.\n",
    "    @param temperature: float. Higher makes the model more indecisive. \n",
    "    @param num_sample: int. How many sentences to generate.\n",
    "    @return string[]. List of num_sample sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # See https://huggingface.co/transformers/main_classes/configuration.html#transformers.PretrainedConfig\n",
    "    # For some reason, num_return_sequences is ignored.\n",
    "    config = {\n",
    "        'max_length': 1500,\n",
    "        'do_sample': True,\n",
    "        'top_k': 50,\n",
    "        'temperature': temperature\n",
    "    }\n",
    "    generator = pipeline('text-generation',model=MODEL_OUTPUT_DIR, tokenizer='gpt2',config=config)\n",
    "    \n",
    "    sentences = []\n",
    "    for i in range(num_sample):\n",
    "        sentence = generator(prompt)[0]['generated_text']\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sentence(filewriter, sentence):\n",
    "    filewriter.write(sentence)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = Neurons are\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons are assumed to have same function\n",
      "Neurons show clear neural function but different functions\n",
      "Sensory neurons vs non-neural: no V(t) neurons, only V(t) neurons and V(t) are\n",
      "\n",
      "Neurons are dominant Neurons, Neurons are non- dominant Neurons, Neurons dominant, Neural, Neurons dominant, Neural, Neurons dominant Neurons dominant Neurons dominant, Ne\n",
      "\n",
      "Neurons are interneuron-species. \"Neural activity is directly interneuron-species in decision-making in Ca2+-dependent\"\n",
      "Neurons are decision making with Ca2+- insensitive, and non-ne\n",
      "\n",
      "Neurons are organized into 1 sub-sect-brain, sub-sect-brain-behavioral, interneuron-level dynamics\n",
      "Neural, interneuron-level, interneuron. Temporal-level interneuron-\n",
      "\n",
      "Neurons are not interneuron-based\n",
      "Data extraction. Neurons are generated by using F(L, Neurons and their interneuron model. Each neuron has its own postsynaptic input is its own Neurons\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = Neurons are\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons are more interconnected, there is the interneuron model with the network\n",
      "Neurons are interneuron model: Neurons connectome connectome with interneuron.\n",
      "Neural models, not necessarily necessary for causality\n",
      "\n",
      "Neurons are strongly expressed in Na+- insensitive Neurons\n",
      "Neural response, Neurons strongly expressed in Vth response\n",
      "Neurons expressed in Ca+ insensitive. Neural response to Vth response, Vth response for\n",
      "\n",
      "Neurons are connected to each motor neuron, as opposed to non-neurons\n",
      "Human knowledge discovery process very much in a monotonous pathway, eigen-controllability-based. Higher levels more structured brain\n",
      "More complex patterns\n",
      "\n",
      "Neurons are involved in the multi-modal system\n",
      "Decision making mechanisms at the system-level. Neuron-level. Closed loop, multi-modal, multi-modal decision making: Neural decision-making making during\n",
      "\n",
      "Neurons are assumed to be causality relationships\n",
      "Multi-modal relationships among different ontologies\n",
      "Neurons and monotonous Neural Vth (neural, somatic, non-vth and motor relationships, interneuron\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = Neurons are\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons are also connected to the nervous system\n",
      "Neural response. E.g. the interneuron network between the nervous system are most active. Neuron-specific. Neural response. Most active neurons is for NNeural response\n",
      "\n",
      "Neurons are organized into 7Neurons (with 0 interneuron separations, one by one by oneneuron)\n",
      "Neural, motor, visual, or interneuron, one by one, is connected by interneuron\n",
      "\n",
      "Neurons are found in the Caenorhabditis elegans.\n",
      "No V-Neuroethans, no more Neurons Neurons show clear defined V-Neural focus. Each pathway has its own neuro more Ne\n",
      "\n",
      "Neurons are missing in Neurons in vertebrate models, but missing in vertebrate models with multi-nethology single-neural dynamics\n",
      "Neural--Lm model: Neuron-level dynamics directly\n",
      "Neural-\n",
      "\n",
      "Neurons are the primary motor neurons, secondary is to sensory.\n",
      "Neurons in a hierarchical hierarchy? No hierarchical decision making. Multifurons sub-modifying the hierarchy of motor circuits. Neurons in a hierarchy. Each\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = The brain is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brain is a small set of interconnected network that constantly changing digital mosaic of neurons; the behavioral data is highly interconnected\n",
      "This is aproprioceptionally based on complex network as it responds to various neurotransmitter setups across the multiple-based setups\n",
      "\n",
      "The brain is not the same as the actual [behavior, behavior, the same as the model as the actual]\n",
      "Behavior and behavior The same as the same as the model as the same as the network as the network as the representation of the\n",
      "\n",
      "The brain is as large as a fixed point object as possible, as the multiple fixed point solutions (for example, the mean, fixed point solutions). 2.\n",
      "The interneuron mean (for now non fixed point solutions) are usually fixed point\n",
      "\n",
      "The brain is the most active sharing state, the most interconnected state (Ca 2+), will be able to respond\".\n",
      "Brain-wide, Interneuron Collaborative Brain-wide Collaborative Collaborative Collaborative Brain-wide Collaborative Collaborative\n",
      "\n",
      "The brain is still the same, but a lot more controllability decisions are expressed in the abstract\n",
      "Neural models in general. Just like brain patterns as patterns. See The Neural Network for each representation of each representation. https://dGm\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = The brain is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brain is the primary visual cortex, but also contains many interneuron interactions with the text. \"instructions that are most varied, are the interneuron relationships, need to understand flow of text, each having its own spatial / tempor\n",
      "\n",
      "The brain is actually doing the talking. A machine readable representation of what is the brain reading is actually thinking when we are reading, or writing (or reading) the text. V(t) is the V(t) constant, where t is\n",
      "\n",
      "The brain is a machine readable representation of the human brain, and there are many thousands of neurons interconnected.\n",
      "Computational representation of The goal is to represent represented by many sub-neuron-modelling tricks\n",
      "Prelational terms used for\n",
      "\n",
      "The brain is a multi-modelling interneuron agreement, but at the most 10 to the highest sensitivity is one based on interconnectedness. Each row is 4.\n",
      "Behavior: full-map - Multif-modulus interneuron\n",
      "\n",
      "The brain is very interesting\n",
      "BrainMap: Aspect: Neural-Interneuron Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collaborative Collabor\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = The brain is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brain is full of unknown untapped potential. Only the network is full of unknown traj\n",
      "The GCaMP4/i search is full of unknown traj\n",
      "There are 2GB in our network of data is the GCaMP2\n",
      "\n",
      "The brain is a mosaic of neurons, but each has its own dynamics, one by one by one. Each has its own dynamics, but usually best responds to different stimulus.\n",
      "Neural network of a mosaic of interconnected network. Each mosaic of neurons\n",
      "\n",
      "The brain is the most active participant in the organization of the organization of the network of an integrated network.\n",
      "Neural interneuron control theory framework for the framework for presenting the neural network of an open science approach to presenting framework for presenting the framework\n",
      "\n",
      "The brain is indeed full of information-sharing agreement.\n",
      "This will not be full of information sharing standard will be more connected networks, especially in a highly interconnected network such as the One Network-wide Paradigm Paradigm on Network-Based Network-\n",
      "\n",
      "The brain is still the same as it always is.\n",
      "Brain-wide interconnected\n",
      "All synapses turn in and out to be interconnected. This makes for one. Most active synaptic connections make it respond to different temporal contour of the interneuron\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = The number of neurons in the brain is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neurons in the brain is the ratio of the 1 to the number of N/1. This constant is expressed as the log n+1.\n",
      "The number of the log n+1. The number of neurons in the network based\n",
      "\n",
      "The number of neurons in the brain is fixed, but can change over time.\n",
      "The fractional-resisational relationships between neurons, how they respond to different timestepaper timestepaper timestepaper modulation\n",
      "The fractional\n",
      "\n",
      "The number of neurons in the brain is constant, if any are used over time\n",
      "lower 20th percentile\n",
      "Mechanism: stochastic inhibition in rodents using high threshold\n",
      "Mechanism: inhibition while reading threshold for inhibition\n",
      "Current blocker\n",
      "High threshold\n",
      "\n",
      "The number of neurons in the brain is the same for all the knobs - medium, medium, medium, medium, different, one, one, and two, one, two, one, two, one, two, one, two, one\n",
      "\n",
      "The number of neurons in the brain is most sensitive to manipulations with the input. The closer to the number that is seen in the noise, usually 1 param, can be inferred from the mean param, and not the noise, the fractional log\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = The number of neurons in the brain is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neurons in the brain is the ratio of the ratio of the mean fluorescence from different synapses. The synaptic connections between the neurons, which is the loudness and what is the loudness.\n",
      "the loud interneuron interne\n",
      "\n",
      "The number of neurons in the brain is controllability in controllability space. The more controllability + the number of controllability + the number of controllability + the number of controllability + the fractional max.\n",
      "\n",
      "\n",
      "The number of neurons in the brain is tightly coupled, with the number of knobs that are coupled to the number of knobs tightly coupled neurons, defined modulus, based on the ratio of the number of knobs that are tightly coupled to the\n",
      "\n",
      "The number of neurons in the brain is controllability-based restrictions.\n",
      "The synaptic connections between neurons controllability restrictions are in the behavioral tricks. Given multiple connections, each one can control the other over 20 mborders.\n",
      "modelling\n",
      "\n",
      "The number of neurons in the brain is controllability (Kunert 2014)\n",
      "theta and kunert 2014\n",
      "Kunert 2018\n",
      "Number of controllability depend on the accuracy in predicting the decision making behavior - the ratio of\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = The number of neurons in the brain is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neurons in the brain is also controllability assessment will likely be an exponential exponential growth in the next 2 weeks or less. Higher The Normies of 2.\n",
      "Whole number of neurons vs Higher TheWhole number.\n",
      "Wh\n",
      "\n",
      "The number of neurons in the brain is controllability-based knobs. What's the knobs of controllability depend on the number of knobs, the number of knobs of neurons controllability. A network's given knobs\n",
      "\n",
      "The number of neurons in the brain is controllability dynamics being controllability dynamics being imputed.\n",
      "The number of controllability dynamics being imputed.\n",
      "themodelling the dynamics being imputed.\n",
      "observable by the open\n",
      "\n",
      "The number of neurons in the brain is the fractional-annealing the variance (F(t) of Vth param, which is theta, which represents the fractional-resis theta and the log 2 and Ï† is expressed\n",
      "\n",
      "The number of neurons in the brain is fixed point, the number of controllability parameters is fixed point -- the number of controllability parameters depends on the number of controllability parameters. What is the controllability parameters is controllability\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = The universe is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The universe is at a higher resolution of a set of fixed point.\n",
      "This is the resting potential\n",
      "Theta, theta 2\n",
      "Calhoun 2009 https://workflowy.com/author/Arnold Causal evidence accumulation theory:\n",
      "\n",
      "The universe is a small place, and we are smallOne we seem, but we are smallOne we are smallOne we are One dimensionality principleIf we can use this knowledge too, then we can claim that our small One, our partial distribution\n",
      "\n",
      "The universe is a small-DG (zero, one, 10, 20, 25, 30)\n",
      "One by one (zero, one, two, two,three, two, multiple, two, multi-dimensional, multi-dimensional,\n",
      "\n",
      "The universe is a good place to store billions of small-world observations\n",
      "Just some small-threshold perturbation. Multifold space. See small-world, large-world\n",
      "This is an extension of \"onset\". Multif\n",
      "\n",
      "The universe is at its most profound limit\n",
      "I want to know what my limit is.\n",
      "Unsorted cycles of periodic table cycles (not always a constant, sometimes used to measure cycles, usually 1 and 2)\n",
      "Discord format / h\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = The universe is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The universe is full of surprises, unexpected sources of instability. Change often comes with the release, some\n",
      "P (Future work)\n",
      "There are many choices, some - there are many choices, some will be.\n",
      "Theta: releases some potential\n",
      "\n",
      "The universe is full.\n",
      "Spike, worm, worm, small, big, worm, larger than expected\n",
      "Spike, worm, smaller than expected\n",
      "Spike, worm, larger than expected\n",
      "Spike, worm, larger than expected\n",
      "\n",
      "\n",
      "The universe is full of interconnected stories\n",
      "Novelty\n",
      "Novelty in concept/ concept/determine.\n",
      "Mystructure-function causality relationships across time. Acknowledgement. Collaborative reading: https://www.legans\n",
      "\n",
      "The universe is a complicated place. We may be complicated.\n",
      "See https://www.youtube.com/watch?vaultier PUBLIC domain / small set of resources\n",
      "OpenWorm https://openworm.org/docs.google.com\n",
      "\n",
      "The universe is our own internal- and interneuron resolution\n",
      "Me. I think they create it.\n",
      "In terms of fixed point solutions\n",
      "There are those that I want.\n",
      "Interneuron resolution problems that can be solved by just some number\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = The universe is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The universe is full of potentials\n",
      "The most varied potentials\n",
      "We have a full potentials potentials potentials potentials potentials potentials potentials potentials potentials potentials potentials potentials potential potentials potentials potentials potential\n",
      "\n",
      "The universe is about to end.\n",
      "\"If it were made a small one, its beginning is not at a very small one, we can have, we can control its beginning\" -- the beginning principle -- as a whole -- should start principle does\n",
      "\n",
      "The universe is 242574\n",
      "Future work\n",
      "Theories presented in the literature\n",
      "Suggestions for making\n",
      "Problem definition / solutions\n",
      "Solution https://www.neuronsensource.com/docs.google.com/document/d64\n",
      "\n",
      "The universe is our only constant, not the sum of all the fixed, and infinite units of fixed, and the fractional units, and unit of the logarithm of the units, in units, such that all units are fixed.\n",
      "\n",
      "\n",
      "The universe is full of potential lies outside our capacity\n",
      "This is our capacity. Too full to know potential to allow us to exist. Too full to think. Too full to account. Too full to account to create - full of potential / finite resources\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = Math is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math is probably biased towards Math. Math is probably not even skewed Math)\n",
      "Scipy Math. Math output skewed Math. Multifold Math. Multifold Math output skewed Math. Math output skewed Math. Math result skewed Math. Math\n",
      "\n",
      "Math is hard to make sense, andMath is Math is just Math is the integral of Math is a function of integrature of logarithm\n",
      "Calcium imaging\n",
      "Calcium imaging\n",
      "Calcium recording + noise transmission with polarities decaying\n",
      "\n",
      "Math is really just a set of math formulas that captures the essence of a sentence. The math problem is to problematode math on these into something readable text. Math functions represent thousands of math formulas that represent thousands of math tricks to get a good\n",
      "\n",
      "Math is a noisy log\n",
      "Me: Math log scale / logarithm of logarithm of logarithm of logarithm of logarithm of logar logarithm of logar logarithm\n",
      "\n",
      "Math is a noisy noisy thing\n",
      "There are timestepaper and other noisy timeseries\n",
      "OpenWorm for w/ Ntrans people / no longer have a lot of neurons running in the log\n",
      "Linear logarithm. Math functions\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = Math is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math is Math / Math / Math / Math = Math Math / Math where Math / Math / Math / Math / Math = Math / Math / Math in Math / Math - Math in Math in Math and Math + Math in Math. Math might be\n",
      "\n",
      "Math is a noisy system, can be chaotic, causing the system to behave strangely when it is switched on, causing memory to stall\n",
      "What is the system interpretable as the logarithmological assertions? binary logarithm of logar\n",
      "\n",
      "Math is another big property of Math / Math's logarithm of logarithm of logarithm. logarithm logarithm.\n",
      "What. Logarithmics: logarithm of logar\n",
      "\n",
      "Math is probably the most important thing that Math is probably best used for approximation: approximation based on variance of log. We need to use approximation as a function of variance of logarithm.\n",
      "Just variance logarithm logarithm\n",
      "\n",
      "Math is the derivative of variance, not the mean over all the logarithm logarithm logarithm logarithm log logar logarithm logar logarithm\n",
      "1 log log log logar log\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = Math is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math is another good approximation of the truth, but I don't have any way of making this simple yet, and Math is still useful for complex functions\n",
      "But Math is another useful for Math for complex functions Math functions used for complex functions can have Math\n",
      "\n",
      "Math is still useful to scientists. Math formulas and Math formulas stay valid? Math formulas stay valid if at least 10% of a century ago and Math formulas get Math formulas to convert to Math texts. Math formulas get Math functions\n",
      "What to do you\n",
      "\n",
      "Math is the sum of all logarches, not logaracts, not logaracts\n",
      "Logaracts logaracts\n",
      "Thus, logaracts logaracts\n",
      "For each phase, logaracts logarithm of logar\n",
      "\n",
      "Math is probably not a good fit for all the data. Math will show that 1/(n/m^-1) log log logarithm\n",
      "Me: Math is logarithm256 log log log logarithm\n",
      "Me\n",
      "\n",
      "Math is still the same thing as Math\n",
      "Get Mathies/(v^2*logits logits logits pi) cos logits logits logits logits cos logits logits cos logits logits logits logits logits\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = Science is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science is probably the most active journal in the tech community by far. https://manipulation.google.com/document/d/10.10.136506/attribution-google.com/document/d/6f01\n",
      "\n",
      "Science is probably the single biggest scientific push yet.\n",
      "Overall classification: Coreference for categorizing papers, based on data. Each paper has its own section (data-sharing for each paper, for each paper), then tackling the many different questions.\n",
      "\n",
      "Science is probably the single most important thing to do if we aim at making tech policy. 1A though, it does not actually exist but has a global single-handedly making science policy. It requires a set of rules, and the whole data set\n",
      "\n",
      "Science is always changing, and IRL has plateau potential. Theta/dt in Vth. https://abs.google.com/scipy.com/adv-to-vth.\n",
      "LockPoint Vth ontology open science\n",
      "\n",
      "Science is growing increasingly hampered by lack of resources\n",
      "But when you make a discovery effort. As the discovery effort deep enough you save time in research effort\n",
      "Warrington 2019 - Craven 2015 does a rather meta-analysis paper over many experts argue that\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = Science is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science is something that everyone should be experts should start on. Collaborativescience is something that everyone should start on.\n",
      "Fundamental science. Collaborative science is important and should start time = time = start time = end time = reuse = reuse.\n",
      "\n",
      "Science is actually generating billions of dollars per year in non-spikes, and computing in spiking speed.\n",
      "But, multi-modal. Mult-modal. Mult-modal. Multifurons, multi-modal.\n",
      "\n",
      "Science is probably not the best fit for a global search strategy, because it allows search goals to be.\n",
      "Not a good fit for a global search, because it allows search goals to be defined, but for the most likely doing the searchable in\n",
      "\n",
      "Science is a discovery pipeline that spans across a human-readable medium.\n",
      "What. Literature to see following: Neuroscience - by Drosophila by Drosophila by worm, showing exponential growth. Most interested in Drosophila by worm\n",
      "\n",
      "Science is not well studied, and I think that it lacks dynamical potential. I still a lot of data\n",
      "The problem. I have more complicated. I need data. Data Science has plenty of complexity, and requires reading paperas. I have\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = Science is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science is not about tech-related\n",
      "Main point. No Vogel's are the \"science\"\n",
      "My guess. tech-related. AIZ. A-ish biased. https://docs.google.com/acceleration.com\n",
      "\n",
      "Science is not a research effort. It is a set of literature to research effort. Each paper cited and cited thousands of times. Papers need to be cited. Literature cited to see the list of references.\n",
      "https://docs.google.com/\n",
      "\n",
      "Science is very limited. Most papers focus on single-function papers. Papers focus on single-function papers. Papers focus on single-function papers, multi-scipy science. Papers focus on causality relationships. Papers focus on causality relationships\n",
      "\n",
      "Science is a long time coming. I need more structured survey data and ask question. Need structured questions like question for question. https://docs.google.com/accelerate.com/advanced.com/data-assessment-with\n",
      "\n",
      "Science is the science of the art, and the science of the science of the subject, and the science of the science of being.\n",
      "One typically undertaken by humans and animals, with some kind of cognitive burden. Too interconnected to require many resources to\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = The world is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is full of interconnected interconnected data centers. Software projects can be interconnected\n",
      "Wikipedia\n",
      "Wikipedia 2019 - On-Mainstream (OpenWorm https://openworm.org/wiki/OpenWorm/onorm https://www.open\n",
      "\n",
      "The world is full of data sources: wikipedia\n",
      "Future work\n",
      "Quantify data sources\n",
      "quantify data sources\n",
      "Data-driven efforts\n",
      "Future work\n",
      "Ablation's goal\n",
      "Status. Good definition of data standards\n",
      "Status. NGN\n",
      "\n",
      "The world is full of human knowledge, full of interconnectedness. Completeness.\n",
      "Warrington 2019 - A collection of papers (the core texts\n",
      "Warrington 2019: \"The global overview (Warrington 2019) summarized: 4 books for\n",
      "\n",
      "The world is full of data replicas.\n",
      "Gordap. Multisample the datagatherer approachGord The single-level knobs. Each dataset has its own dynamics, each with its own dynamics.\n",
      "Data Replication\n",
      "G\n",
      "\n",
      "The world is full of interconnected datasets. More data can readily fits within a single-dimensional datasphere, but requires more interneuron interactions, more interconnected datasets.\n",
      "Dataset infers interconnected datasets\n",
      "Just like NIFurcation in\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = The world is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is saturated and the fractional-resisolution is reached in CVD-resolved Vth order global\n",
      "This fractional-resisolution is reached in the Vth order\n",
      "This fractional-resisq of the V\n",
      "\n",
      "The world is moving inexorablyoward toward understanding of what makes this reality real, and can and should be fully understood as a more useful century can we can make clearer notion of what the \"truth\".\n",
      "Ablation 2014: One goal is\n",
      "\n",
      "The world is full of biophysical data, digital data, and the tech for biophysical data-driven data visualization\n",
      "Just some data (not global, just biophysical datasets\n",
      "Somedaset of most commonly used data sources such as the sensor\n",
      "\n",
      "The world is now known as \"the cytoplasm.\"\n",
      "There are 4. V_th (vth)\n",
      "A*/5 (1)\n",
      "The interconnectedness of the network of Vth (1)\n",
      "What are the\n",
      "\n",
      "The world is rapidly changing. More interconnected resources\n",
      "OpenSource\n",
      "OpenSource Collaborative project\n",
      "Open source project. Collaborative project\n",
      "Open Source\n",
      "OpenSource\n",
      "Open source\n",
      "Open source\n",
      "GCC â€“ Collaborative project\n",
      "Open Knowledge base for\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = The world is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is full of activity and activity indicators. https://www.neuronstom.org/showall%2FlxrGaGa2SEjnl6yRMUb3TIFcn8zk6f264AYi\n",
      "\n",
      "The world is full of interconnected resources\n",
      "One is full-stack softwarestack. A distributed softwarestack is fullstack is fullstack for software (as opposed to traditional \"stack\" fullstack for software.\n",
      "A full-stack is fullstack for\n",
      "\n",
      "The world is full of interconnected resources\n",
      "Open science\n",
      "Open science\n",
      "Open data science\n",
      "Gap.org/docs.google.com/opendata-dynamical-structure-science.org/advanced_labs.google\n",
      "\n",
      "The world is full of potentials resources\n",
      "Just like abundance of resources, abundance of options. Humans too have the tools to make infinite potentials\n",
      "But different approaches to creating\n",
      "Human resources = finite resources, infinite potentials infinite potentials\n",
      "But\n",
      "\n",
      "The world is full of interconnected computers interconnected physical systems\n",
      "Open science literature\n",
      "open science datasets\n",
      "What do you want to know about open science papers\n",
      "OpenWorm.org/openworm/docs. https://openworm/doc.org\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = What is the next big thing?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the next big thing? What is the next big thing. What are the state space used? What is the \"useful for dR/dV/dt. dR / dR / dR/dt. What is the\n",
      "\n",
      "What is the next big thing? A neural network analysis framework for finding AIZ structured data, where the question is \"how to find neurons\"\n",
      "Neural analysis of the network state transition. Given a decision, how the system responds to \"truth\n",
      "\n",
      "What is the next big thing? The next big thing: A categorization of what's going to happen next. A hierarchy of stuff. \"c What is going to happen next? What entities to categorize (a) should be represented by different\n",
      "\n",
      "What is the next big thing? In-vivo evidence-based evidence-based decision making? Vth (VA). What is the ontological claim? https://docs.google.com/aconda.com/document/d/6\n",
      "\n",
      "What is the next big thing? How many minutes to tweak in the parameters, add timestepaper\n",
      "Experiment execution\n",
      "Whole-life balance? If the experiment is over, make a simple decision, want to quantify usefulness. If too\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = What is the next big thing?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the next big thing? What is the ontological claim. What are some ontological claim to do when no ontological claim is made is actually a claim. What is the ontological claim being made? What is the ontological claim being\n",
      "\n",
      "What is the next big thing? The ontology of the ontological claim. What is the ontology of thing we want, what is the ontology being ontologically useful? How does it seem to be when we claim that something is ontologically\n",
      "\n",
      "What is the next big thing? Summarize the number of repetitions in repetitions in a step, then divide by 2 by the number of repetitions per step. Summarize the number of repetitions in the number of repetitions.\n",
      "\n",
      "What is the next big thing? What is the next big thing? What are the \"world\" (document type, function, hierarchy of functions, hierarchy of functions, hierarchy of functions, hierarchy of functions, same hierarchy of states? What are the\n",
      "\n",
      "What is the next big thing? What will it be used for synchronous moving parts of the stick to one end, and others will happen in the middle. How many timestepaper-dependent. How many timestepaper-dependent.\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = What is the next big thing?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the next big thing? What is the next big thing? What are the \"assume nothing\" going to look like?\n",
      "What are the \"assume nothing\" going to look like? A(t) decaying exponential growth in a\n",
      "\n",
      "What is the next big thing? \"What are some hierarchical structure of the hierarchy of the distribution of hierarchical units of claim the \"examples\"\n",
      "What is the \"examples\" to consider\"\n",
      "What are some hierarchical hierarchy of \"examples\n",
      "\n",
      "What is the next big thing? What are some kind of \"level mapping problem in functional neuroscience to do we have\" (fwd / some kind of \"behavioral mapping\" problem. E.g. hierarchy of slides (functional vs non-\n",
      "\n",
      "What is the next big thing? What are the \"workflowy.com/animated doc (for sharing) good for a \"workflowy.com/animated doc for sharing? \"flowy/animated doc for sharing.\n",
      "\n",
      "What is the next big thing? What are some synapse going to be?\n",
      "One that we doing \"synapse after synapse\" (synapse-by-synapse-by-synapse-synapse-by-synapse-\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = Information should be\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information should be used to guide keeper and tech. â€“ Log in to / not keeper\n",
      "Gleeson 2005.\n",
      "Log in to https://docs.google.com/document.d/15.0 (info.google.com/document\n",
      "\n",
      "Information should be used carefully.\n",
      "Data cleaning should start with creating guide-based on transitions: npy - no transition information. https://docs.google.com/document.google.com/document/d/1f be used before making\n",
      "\n",
      "Information should be used when making a decision.\n",
      "Main point. How does discovery algorithm update guide. Each update guide. https://docs.google.com/document.com/d/2856998849\n",
      "https://www.google.\n",
      "\n",
      "Information should be used carefully.\n",
      "What is the focus be? <a href=\"https://docs.google.com/document.com/d/6f0f41549, <a>\n",
      "Focus on the Author > https://docs\n",
      "\n",
      "Information should be taken into account when making a decision making step:\n",
      "Data sources should be kept separate from human knowledge (e.g. reading\n",
      "https://docs.google.com/document/d/6f.google.com/document\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = Information should be\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information should be used.\n",
      "If not found on-browser when making\n",
      "Keyword IDENTIFIA -> NOT included in this if not found on this entry\n",
      "Keyword -> Authoritative. https://en go back. https://docs.google\n",
      "\n",
      "Information should be used for eigenvalue+ not guessing.\n",
      "Me. Meh. I think so. Based on database. https://www.backward.com/d be used to make sure you have https://workflowy.com/\n",
      "\n",
      "Information should be taken into account when making this assumption.\n",
      "Open questions to questions be answered: https://docs.google.com.com/document/d/1k?id=0f0849\n",
      "This be? https://openquest\n",
      "\n",
      "Information should be used with this info.\n",
      "Trace the references to RFP references be used. For timestepaper transition information be used. https://www.ncbi.nlm.nih.gov/content.\n",
      "Trace the\n",
      "\n",
      "Information should be used as a guide to decision making with regards to software development - https://docs.google.com/document.google.com/d/12.google.com/document/d/a0f49f4949.\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = Information should be\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information should be used to enhance user understanding.\n",
      "Key points: For each step in learning: [pheno]\n",
      "Identify information extraction techniques, i use this info.\n",
      "Key decision making information be taken immediately. Create guide. From top 2\n",
      "\n",
      "Information should be undertaken first.\n",
      "Key points to understand this information be used when making: multi-Tasked / multi-Tasked work. Integration-level-based. Make sure that setup clear, start on a clean, simple for all.\n",
      "\n",
      "Information should be used with OpenWorm https://docs.openworm.com/install.com/linux-reference/fullname.com/manually use this be used. https://docs.google.com/install-linuxinfo.\n",
      "\n",
      "Information should be taken into account.\n",
      "https://docs.google.com/accelerate.com/author/d.google.com/author/Marco.com/Calhoun. https://docs.google.com/openworld.\n",
      "\n",
      "Information should be carefully undertaken before making a decision.\n",
      "My example. Each representation type is described in detail. https://docs.google.com/document.com/document/d/1B.g.d be carefully undertaken. If too\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.5. Prompt = I like\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like Q: What will Q: Would you like to have Q: Would you like Q: Would you like the final state to have its own sentence structure\n",
      "I like your formulation. It's a summary of the definition of semantic terms. Q\n",
      "\n",
      "I like how they pick them. I like the picker.\n",
      "My preference. I don't see any 2 options because of the focus on 1 output. If interested, I can experiment and compare them too. Papers 1 and 2 should start on\n",
      "\n",
      "I like the framework for presenting my papers. I think it's simple enough that its usefulness.\n",
      "Open question --- I like how many examples that open question. Can't find references -- I will probably change over time. https://docs.google.\n",
      "\n",
      "I like them so much more options, but I find that they don't seem more intuitive but still. I willymu terms too. I like reading intro but unfamiliarity to reading\n",
      "Key findings: I like having intuitive interneuron vs inter\n",
      "\n",
      "I like the idea of hierarchy, but don't like hierarchy too fixed. I like the framework - hierarchy: hierarchical systems of representation, hierarchical systems of causality relationships. I like hierarchy very much, but limited to just hierarchical relationships. I want hierarchy\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.7. Prompt = I like\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like the idea of it.\n",
      "Need more examples. Need more examples and comments. Craving it. Go through all the examples. https://docs.google.com/academic.com/reference/d/1149f55.\n",
      "\n",
      "I like Kato's example - can use Kato's example first - can't just assume that my objective is objective is to justify my decision making standards.\n",
      "I would like how you guys think it - Kato's proposal - you should just\n",
      "\n",
      "I like this idea of having this analogy because it's easy to visualize the functional connections between neurons, and not just behavioral\n",
      "Also worth noting that this: I like the conceptualizations. Just to get something out there that's informative when it goes towards\n",
      "\n",
      "I like reading patterns, but I get what I want.\n",
      "One that I like the framework. It's nice that everyone has a framework and I can think of thinking about using this. I get the idea that if I want to start thinking about\n",
      "\n",
      "I like reading https://docs.google.com/scipy.com/acom/author/shliht.com/adam.com/~demarcation/scipy.com/reference/presentation/\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "Temperature: 0.9. Prompt = I like\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like thinking about moving forward. For now. I want to experiment. I think that much easier over time. For now, just learn weights and measures. I_ext - useful to have background knowledge base. I want to dig my own conclusions\n",
      "\n",
      "I like how they break down causality relationships pretty crude to model. I like the framework of categorization.\n",
      "Decision making\n",
      "Just to show how many causality relationships: 2 (I think it's just categorize organisms -- organism vs cy\n",
      "\n",
      "I like / dislike modulatory. I don't need a reference. Too interconnected wikipedia\n",
      "If more than 1: insufficient, I will dismiss potential conflicts. Maybe I need a proposal by Kato: \"completes existing conflicts\".\n",
      "Suggest\n",
      "\n",
      "I like XML - good fit. I like ML for ML for structured presentations.\n",
      "This is a good fit because it allows for single-use presentations. I was thinking - I like structured slides. I like categorize slides. I like categor\n",
      "\n",
      "I like Eli's decision making approach, but I don't think it's wise for Eli's decision making decisions. Eli's approach, argues that \"ifold. I'm biased, I prefer biased Eli\" because there is no clear pathway to making\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "\"Neurons are\",\n",
    "\"The brain is\",\n",
    "\"The number of neurons in the brain is\",\n",
    "\"The universe is\",\n",
    "\"Math is\",\n",
    "\"Science is\",\n",
    "\"The world is\",\n",
    "\"What is the next big thing?\",\n",
    "\"Information should be\",\n",
    "\"I like\"    \n",
    "]\n",
    "\n",
    "NUM_SAMPLE = 5\n",
    "GENERATED_SENTENCES_PATH = \"data/generated_sentences.txt\"\n",
    "with open(GENERATED_SENTENCES_PATH, \"w\", encoding='utf-8') as outfile:\n",
    "    for prompt in prompts:\n",
    "        for temperature in [0.5, 0.7, 0.9]:\n",
    "            write_sentence(outfile, \"-------------------------------\\n\")\n",
    "            write_sentence(outfile, f\"Temperature: {temperature}. Prompt = {prompt}\\n\")\n",
    "            sentences = generate_sentences(prompt, temperature, NUM_SAMPLE)\n",
    "            for sentence in sentences:\n",
    "                write_sentence(outfile, sentence + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
